{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes a dataframe and an attribute as input. Computes the entropy of splitting\n",
    "the dataset on the values of the attribute.\n",
    "\"\"\"\n",
    "def gain(data,D):\n",
    "    m = len(data)\n",
    "    if m <= 1: return 0\n",
    "    values, counts = np.unique(data[D],return_counts=True)\n",
    "    probs = counts / m # probabilities of D's values\n",
    "    ent = sum([prob * entropy(data[data[D]==val]) for prob,val in zip(probs,values)])\n",
    "    return entropy(data) - ent\n",
    "\n",
    "\"\"\"\n",
    "Takes a dataframe as input. It computes the entropy of a target varaible specified globally\n",
    "\"\"\"\n",
    "def entropy(data):\n",
    "    m = len(data)\n",
    "    if m <= 1: return 0\n",
    "    value, counts = np.unique(data[target], return_counts=True)\n",
    "    probs = counts / m # probabilities of y's values \n",
    "    n_classes = np.count_nonzero(probs)\n",
    "    if n_classes <= 1: return 0\n",
    "    return -sum([p * log(p,2) for p in probs])\n",
    "\n",
    "\"\"\"\n",
    "Takes a dataframe and attribute as input. Returns the normalized information gain\n",
    "of splitting the data on the attribute.\n",
    "\"\"\"\n",
    "def gain_ratio(data,D):\n",
    "    values,counts = np.unique(data[D],return_counts=True)\n",
    "    get_ratio = lambda x: (x / len(data)) * log((x / len(data)),2)\n",
    "    intrinsic_value = -sum([get_ratio(x) for x in counts])\n",
    "    return 0 if intrinsic_value == 0 else gain(data,D) / intrinsic_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learn and Predict Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This algorithm recursively learns a decision tree to fit the input data.\n",
    "ID3 is used when the Information Gain is passed in as the importance function\n",
    "and C4.5 is used when the Gain Ratio is passed in. \n",
    "\n",
    "Input:\n",
    "data - a pandas dataframe of data to utilize\n",
    "target - the string that is the name of the target column in the data dataframe\n",
    "attributes - the string list of data's column/header/feature names (minus the target)\n",
    "level - passed in as 0. Increments upon recursion to help debugging\n",
    "importance - a function which takes the data and an attribute in the dataframe as input/\n",
    "        - if 'gain' is used, this algorithm becomes Quinlan's ID3 algorithm\n",
    "        - if 'gain_ratio' is used, this algorithm is more like Quinlan's C4.5 algorithm\n",
    "debug - a boolean, where True triggeres print methods to print what the algorithm is doing\n",
    "\n",
    "Output: returns a nested python dictionary corresponding to the learned tree\n",
    "\"\"\"\n",
    "def dtree_learn(data,target,attributes,level,importance,debug=False):\n",
    "    if data[target].all(): \n",
    "        child = {'label':'leaf','value':1}\n",
    "        if debug: print (\"{}All data is pos, returning pos leaf\".format(\"\\t\"*level))\n",
    "    elif data[target].any():\n",
    "        if len(attributes) == 0: \n",
    "            val = 1 if data[target].mode().values[0] else 0\n",
    "            child = {'label':'leaf','value':val}\n",
    "            if debug: print (\"{}no more attributes, returning leaf with majority label\".format(\"\\t\"*level))\n",
    "        else:\n",
    "            best = attributes[np.argmax([importance(data,attr) for attr in attributes])]\n",
    "            child = {'label':best,'children':[]}\n",
    "            if debug: print(\"{}Created {} node\".format(\"\\t\"*level,best))\n",
    "            for val in data[best].unique():\n",
    "                new_child = {'label':'node','parent attr':best,'parent val':val,'children':[]}\n",
    "                new_data = data[data[best]==val]\n",
    "                if len(new_data) == 0: \n",
    "                    val = 1 if data[target].mode().values[0] else 0\n",
    "                    new_child['children'].append({'label':'leaf','value':val}) \n",
    "                    if debug: print(\"{}no more data, returning leaf with majority label\".format(\"\\t\"*level))\n",
    "                else: \n",
    "                    if debug: print (\"{}recursing to next ID3 iteration for {}'s value {}\".format(\"\\t\"*level,best,val))\n",
    "                    new_child.update(dtree_learn(new_data,target,attributes.drop(best),level+1,importance,debug)) \n",
    "                child['children'].append(new_child)\n",
    "    else: \n",
    "        child = {'label':'leaf','value':0}\n",
    "        if debug: print(\"{}all data is neg, returning neg leaf\".format(\"\\t\"*level))\n",
    "    return child\n",
    "\n",
    "\"\"\"\n",
    "This predict function traverses a decision tree according to the features of\n",
    "an input example until it reaches a leaf node. When a leaf node is reached, it\n",
    "returns the label on the leaf, which is the predicted label for the example\n",
    "\n",
    "Input:\n",
    "dtree - a decision tree in the form of nested Python dictionaries\n",
    "example - an input example to classify\n",
    "\n",
    "Ouput: returns 0 or 1\n",
    "\"\"\"\n",
    "def dtree_predict(dtree,example):\n",
    "    if dtree['label'] != 'leaf':\n",
    "        for child in dtree['children']:\n",
    "            if child['parent val'] == example[child['parent attr']]:\n",
    "                return dtree_predict(child,example)\n",
    "    else: return dtree['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Cross-Validation training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cross-validation train datasets and the testing dataset\n",
    "cv_train_files = [\"Mushroom/training_a{}.data\".format(x) for x in ['a','b','c','d','e','f','g','h','i','j']]\n",
    "cv_train_data = [pd.read_csv(tf) for tf in cv_train_files]\n",
    "test_data = pd.read_csv(\"Mushroom/testing.data\")\n",
    "\n",
    "# convert target vectors in all data to binary vectors\n",
    "target, pos_val = 'edible','e' # denoting 'e' and 'p' as the positive and negative classes, respectively\n",
    "for td in cv_train_data: td[target] = td[target].apply(lambda x: 1 if x == pos_val else 0)\n",
    "test_data[target] = test_data[target].apply(lambda x: 1 if x == pos_val else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 with 10-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3 scores on validation data: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# perform 10-fold cross validation\n",
    "id3_trees_n_scores = []\n",
    "for i,valid_data in enumerate(cv_train_data):\n",
    "    merged_train_data = pd.concat(cv_train_data[:i]+cv_train_data[i+1:])\n",
    "    dtree = dtree_learn(merged_train_data,target,merged_train_data.columns.drop(target),0,gain) # gain used: ID3\n",
    "    predictions = valid_data.apply(lambda x: dtree_predict(dtree,x),axis=1)\n",
    "    id3_trees_n_scores.append((dtree,f1_score(valid_data[target],predictions)))\n",
    "print (\"ID3 scores on validation data: {}\".format([score for _,score in id3_trees_n_scores]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ID3 tree's F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# all models have equal cross-validation performance, so selecting the first will do\n",
    "best_id3_tree = id3_trees_n_scores[0][0]\n",
    "\n",
    "# now testing the best model on the test set\n",
    "predictions = test_data.apply(lambda x: dtree_predict(best_id3_tree,x),axis=1)\n",
    "print (\"Best ID3 tree's F1-score: {}\".format(f1_score(test_data[target],predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5 with 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 scores on validation data: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# perform 10-fold cross-validation\n",
    "c45_trees_n_scores = []\n",
    "for i,valid_data in enumerate(cv_train_data):\n",
    "    merged_train_data = pd.concat(cv_train_data[:i]+cv_train_data[i+1:])\n",
    "    dtree = dtree_learn(merged_train_data,target,merged_train_data.columns.drop(target),0,gain_ratio) #gain ratio used: C4.5\n",
    "    predictions = valid_data.apply(lambda x: dtree_predict(dtree,x),axis=1)\n",
    "    c45_trees_n_scores.append((dtree,f1_score(valid_data[target],predictions)))\n",
    "print (\"C4.5 scores on validation data: {}\".format([score for _,score in c45_trees_n_scores]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C4.5 tree's F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# all models have equal cross-validation performance, so selecting the first will do\n",
    "best_c45_tree = c45_trees_n_scores[0][0]\n",
    "\n",
    "# now testing the best model on the test set\n",
    "predictions = test_data.apply(lambda x: dtree_predict(best_c45_tree,x),axis=1)\n",
    "print (\"Best C4.5 tree's F1-score: {}\".format(f1_score(test_data[target],predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
